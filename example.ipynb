{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27ec1693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTitle: Stock market prediction using news headlines\\n\\nAuthors:\\nDate: 05/06/2018\\nShravan Chintha - Role: Cleaning dataset, created relevant visualizations, computed baseline model, implemented TFIDF for data. \\nImplemented Random forest, naive bayes models.\\nAdithya Job - Role: Implemented sentiment analysis, bigram and tri gram models with machine learning models such as gradient boosting,\\nXGboost model and evaluated the model performance.\\n\\nIntroduction:\\n    \\nThe project is about predicting the stock market movement based on the news headlines\\nthat published on a particular day. The news data is collected from Reddit news and \\ntop 25 headlines, ranked based on reddit user votes, are taken on each day. The stock\\nmarket data, DJIA (Dow Jones Industrial Average) of each day is collected from Yahoo\\nfinance. Combined both datasets to process and apply modeling techniques further to \\nget desired results.\\nNatural language techniques such as word clouds, bag of words, ngrams, sentiment\\nanalysis etc., are used to process the data. Also, machine learning techniques \\nsuch as logistic regression, random forest, Naïve Bayes model, gradient boosting,\\nxgboost are applied to predict the outcome variable. A baseline model, logistic \\nregression with bag of words is performed to check the accuracy and use it as a \\nbaseline for our rest analysis. Then, computed accuracies of various model to know\\nthe best performing model among the models we applied. We got the best accuracy for\\nsentiment analysis with xgboost algorithm, able to improve the accuracy to 62.7% \\ncompared to the baseline 46.07%.\\n\\nSteps to be followed to get output/results (accuracy of each model):\\n\\n    1. Download the folder \"GMU-AIT590-ShravanAdithya\" that is submitted and save\\n    on the desktop.\\n    2. Look for the file \"Combined_News_DJIA.csv\" and provide the path of this file.\\n    at line 111 in place of \"C:/Users/shrav/Desktop/Masters/AIT590/Project/Combined_News_DJIA.csv\".\\n    3. Open command promt (cmd)/terminal window.\\n    4. Open the file \"INSTALL\" and copy paste all the content into cmd/terminal, this should \\n    install all the python packages that are required to run this program.\\n    5. Run the file \"project.py\" from the same folder in cmd/terminal. \\n    6. Model accuracy/results of all the models are displayed.\\n\\nALGORITHM\\n\\nSTEP 1 : ACCEPT THE DATAFRAME FORM THE USER DIRECTORY \\n\\nSTEP 2 : SPLIT THE DATASET INTO TRAINING AND TESTING DATASETS\\n\\nSTEP 3 : FOR TRAIN CONVERT THE TOP HEADLINES OF THE DAY INTO  EACH ENTRY OF A LIST, THE LENGHT OF THE LIST IS EQUAL TO THE LENGTH OF THE TESTING DATASET\\n\\nSTEP 4 : FOR TEST CONVERT THE TOP HEADLINES OF THE DAY INTO  EACH ENTRY OF A LIST, THE LENGHT OF THE LIST IS EQUAL TO THE LENGTH OF THE TESTING DATASET\\n\\nSTEP 5 : COUNT THE FRQUENCY OF EACH WORD IN THE DATASET AND CREATE THE CORRESPONDING DATAFRAME TO HOLD THAT INFORMATION\\n\\nSTEP 6 : WORDS APPEARING THE MOST AND THE WORDS APPEARING THE LEAST ARE IGNORED TO MITIGATE THE PROBLEM OF STOPWORDS ANDS LESS APPEARING WORDS\\n\\nSTEP 7 : DEFINING THE BASELINE MODEL (LOGITICS REGRESSION WITH THE BAG OF WORDS)\\n\\nSTEP 8 : FIND THE ACCURACY OF THE BASE LINE MODEL\\n\\nSTEP 9 : EVALUVATE THE BASELINE MODEL WORD IMPORTANCE DISTRIBUTION\\n\\nSTEP 10: COUNT THE FREQUENCY OF THE WORDS BASED ON TFID \\n\\nSTEP 11: IMPLEMENT THE LOGISTICS REGRESSION WITH BIGRAM MODEL WITH TFID TRANFORMATION\\n\\nSTEP 12: EVELUVATE THE LOGISTICS REGRESSION (WITH BIGRAM MODEL WITH TFID TRANSFORMATION)\\'S  TOP TEN AND LAST TEN WORDS\\n\\nSTEP 13: IMPLEMENT THE RANDOM FOREST MODEL WITH BIGRAM MODEL WITH TFID TRANFORMATION\\n\\nSTEP 15: EVALUVATE THE RANDOM FOREST MODEL PERFORMANCE \\n\\nSTEP 16: IMPLEMENT THE NAIVE BAYS MODEL WITH BIGRAM MODEL WITH TFID TRANFORMATION\\n\\nSTEP 17: EVALUVATE THE NAIVE BAYS MODEL PERFORMANCE \\n\\nSTEP 18: IMPLEMENT THE GRADINET BOOSTING MODEL WITH BIGRAM MODEL WITH TFID TRANFORMATION\\n\\nSTEP 19: EVALUVATE THE GRADIANT BOOSTING MODEL PERFORMANCE \\n\\nSTEP 20: IMPLEMENT A LOGISTICS REGRESSION MODEL WITH TFID AND TRIGRAM MODEL\\n\\nSTEP 21: EVALUVATE THE MODEL PERFORMANCE\\n\\nSTEP 22: EVELUVATE THE LOGISTICS REGRESSION (WITH TRIGRAM MODEL WITH TFID TRANSFORMATION)\\'S  TOP TEN AND LAST TEN WORDS\\n\\nSTEP 23: DEFINE THE FUNCTION WHICH EVALUVATES THE POLARITY SCORES OF THE SENTENCE\\n\\nSTEP 24: CONVERT THE TOP HEADLINE IN THE DATAFRAME INTO POLORITY SCORE OF THE TRAIN \\n\\nSTEP 25: CONVERT THE TOP HEADLINE IN THE DATAFRAME INTO POLORITY SCORE OF THE TEST\\n\\nSTEP 26: IMPLEMENT A GRADIANT BOOSTING ALGORITHTM INTO THE NEWLY IMPROVISED DATAFRAME\\n\\nSTEP 27: EVALUVATE THE PEROMANCE OF THE NEW MODEL WITH THE TESTING DATASET\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Title: Stock market prediction using news headlines\n",
    "\n",
    "Authors:\n",
    "Date: 05/06/2018\n",
    "Shravan Chintha - Role: Cleaning dataset, created relevant visualizations, computed baseline model, implemented TFIDF for data. \n",
    "Implemented Random forest, naive bayes models.\n",
    "Adithya Job - Role: Implemented sentiment analysis, bigram and tri gram models with machine learning models such as gradient boosting,\n",
    "XGboost model and evaluated the model performance.\n",
    "\n",
    "Introduction:\n",
    "    \n",
    "The project is about predicting the stock market movement based on the news headlines\n",
    "that published on a particular day. The news data is collected from Reddit news and \n",
    "top 25 headlines, ranked based on reddit user votes, are taken on each day. The stock\n",
    "market data, DJIA (Dow Jones Industrial Average) of each day is collected from Yahoo\n",
    "finance. Combined both datasets to process and apply modeling techniques further to \n",
    "get desired results.\n",
    "Natural language techniques such as word clouds, bag of words, ngrams, sentiment\n",
    "analysis etc., are used to process the data. Also, machine learning techniques \n",
    "such as logistic regression, random forest, Naïve Bayes model, gradient boosting,\n",
    "xgboost are applied to predict the outcome variable. A baseline model, logistic \n",
    "regression with bag of words is performed to check the accuracy and use it as a \n",
    "baseline for our rest analysis. Then, computed accuracies of various model to know\n",
    "the best performing model among the models we applied. We got the best accuracy for\n",
    "sentiment analysis with xgboost algorithm, able to improve the accuracy to 62.7% \n",
    "compared to the baseline 46.07%.\n",
    "\n",
    "Steps to be followed to get output/results (accuracy of each model):\n",
    "\n",
    "    1. Download the folder \"GMU-AIT590-ShravanAdithya\" that is submitted and save\n",
    "    on the desktop.\n",
    "    2. Look for the file \"Combined_News_DJIA.csv\" and provide the path of this file.\n",
    "    at line 111 in place of \"C:/Users/shrav/Desktop/Masters/AIT590/Project/Combined_News_DJIA.csv\".\n",
    "    3. Open command promt (cmd)/terminal window.\n",
    "    4. Open the file \"INSTALL\" and copy paste all the content into cmd/terminal, this should \n",
    "    install all the python packages that are required to run this program.\n",
    "    5. Run the file \"project.py\" from the same folder in cmd/terminal. \n",
    "    6. Model accuracy/results of all the models are displayed.\n",
    "\n",
    "ALGORITHM\n",
    "\n",
    "STEP 1 : ACCEPT THE DATAFRAME FORM THE USER DIRECTORY \n",
    "\n",
    "STEP 2 : SPLIT THE DATASET INTO TRAINING AND TESTING DATASETS\n",
    "\n",
    "STEP 3 : FOR TRAIN CONVERT THE TOP HEADLINES OF THE DAY INTO  EACH ENTRY OF A LIST, THE LENGHT OF THE LIST IS EQUAL TO THE LENGTH OF THE TESTING DATASET\n",
    "\n",
    "STEP 4 : FOR TEST CONVERT THE TOP HEADLINES OF THE DAY INTO  EACH ENTRY OF A LIST, THE LENGHT OF THE LIST IS EQUAL TO THE LENGTH OF THE TESTING DATASET\n",
    "\n",
    "STEP 5 : COUNT THE FRQUENCY OF EACH WORD IN THE DATASET AND CREATE THE CORRESPONDING DATAFRAME TO HOLD THAT INFORMATION\n",
    "\n",
    "STEP 6 : WORDS APPEARING THE MOST AND THE WORDS APPEARING THE LEAST ARE IGNORED TO MITIGATE THE PROBLEM OF STOPWORDS ANDS LESS APPEARING WORDS\n",
    "\n",
    "STEP 7 : DEFINING THE BASELINE MODEL (LOGITICS REGRESSION WITH THE BAG OF WORDS)\n",
    "\n",
    "STEP 8 : FIND THE ACCURACY OF THE BASE LINE MODEL\n",
    "\n",
    "STEP 9 : EVALUVATE THE BASELINE MODEL WORD IMPORTANCE DISTRIBUTION\n",
    "\n",
    "STEP 10: COUNT THE FREQUENCY OF THE WORDS BASED ON TFID \n",
    "\n",
    "STEP 11: IMPLEMENT THE LOGISTICS REGRESSION WITH BIGRAM MODEL WITH TFID TRANFORMATION\n",
    "\n",
    "STEP 12: EVELUVATE THE LOGISTICS REGRESSION (WITH BIGRAM MODEL WITH TFID TRANSFORMATION)'S  TOP TEN AND LAST TEN WORDS\n",
    "\n",
    "STEP 13: IMPLEMENT THE RANDOM FOREST MODEL WITH BIGRAM MODEL WITH TFID TRANFORMATION\n",
    "\n",
    "STEP 15: EVALUVATE THE RANDOM FOREST MODEL PERFORMANCE \n",
    "\n",
    "STEP 16: IMPLEMENT THE NAIVE BAYS MODEL WITH BIGRAM MODEL WITH TFID TRANFORMATION\n",
    "\n",
    "STEP 17: EVALUVATE THE NAIVE BAYS MODEL PERFORMANCE \n",
    "\n",
    "STEP 18: IMPLEMENT THE GRADINET BOOSTING MODEL WITH BIGRAM MODEL WITH TFID TRANFORMATION\n",
    "\n",
    "STEP 19: EVALUVATE THE GRADIANT BOOSTING MODEL PERFORMANCE \n",
    "\n",
    "STEP 20: IMPLEMENT A LOGISTICS REGRESSION MODEL WITH TFID AND TRIGRAM MODEL\n",
    "\n",
    "STEP 21: EVALUVATE THE MODEL PERFORMANCE\n",
    "\n",
    "STEP 22: EVELUVATE THE LOGISTICS REGRESSION (WITH TRIGRAM MODEL WITH TFID TRANSFORMATION)'S  TOP TEN AND LAST TEN WORDS\n",
    "\n",
    "STEP 23: DEFINE THE FUNCTION WHICH EVALUVATES THE POLARITY SCORES OF THE SENTENCE\n",
    "\n",
    "STEP 24: CONVERT THE TOP HEADLINE IN THE DATAFRAME INTO POLORITY SCORE OF THE TRAIN \n",
    "\n",
    "STEP 25: CONVERT THE TOP HEADLINE IN THE DATAFRAME INTO POLORITY SCORE OF THE TEST\n",
    "\n",
    "STEP 26: IMPLEMENT A GRADIANT BOOSTING ALGORITHTM INTO THE NEWLY IMPROVISED DATAFRAME\n",
    "\n",
    "STEP 27: EVALUVATE THE PEROMANCE OF THE NEW MODEL WITH THE TESTING DATASET\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33c738d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70c704c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from textblob import TextBlob\n",
    "#from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbf17f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#author: Adithya Job\n",
    "def analize_sentiment(tweet):\n",
    "    \n",
    "    analysis = TextBlob((str(tweet)))     #defining the function which will find the plority of a sentence\n",
    "    return analysis.polarity \n",
    "\n",
    "news= pd.read_csv('Combined_News_DJIA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25bb2550",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news = news[news['Date'] < '2014-07-15']   # SPLITTING THE DATASET INTO TRAINING AND TESTING\n",
    "test_news = news[news['Date'] > '2014-07-14']\n",
    "\n",
    "train_news_list = []\n",
    "for row in range(0,len(train_news.index)): # CONVERT THE TRAINNG DATASET OF 27 COLUMNS INTO ONE ELEMENT IN THE LIST FOR EACH DAY\n",
    "    train_news_list.append(' '.join(str(k) for k in train_news.iloc[row,2:27]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5f10b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE TABLE OF FREQUENCY WORD DISTRIBUTION (1492, 4733)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jorda\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the baseline model accuracy 0.4607645875251509\n",
      "Top ten words according to the baseline model           Word  Coefficient\n",
      "3728      self     0.627852\n",
      "4647      wing     0.532150\n",
      "2090  hospital     0.531903\n",
      "2392     kills     0.524715\n",
      "284      among     0.516812\n",
      "4387      turn     0.515102\n",
      "762     cartel     0.513453\n",
      "2929  olympics     0.509146\n",
      "1146  damascus     0.505507\n",
      "3585      rise     0.503250\n",
      "Last ten words according to the baseline model            Word  Coefficient\n",
      "3770        sex    -0.540431\n",
      "1163         de    -0.540938\n",
      "990       congo    -0.548367\n",
      "4206     terror    -0.555462\n",
      "4047   students    -0.563786\n",
      "3653  sanctions    -0.569174\n",
      "2100      hours    -0.572354\n",
      "506       begin    -0.602418\n",
      "4301      total    -0.610839\n",
      "3626        run    -0.665524\n"
     ]
    }
   ],
   "source": [
    "vectorize= CountVectorizer(min_df=0.01, max_df=0.8) # DEFINING THE VECTOR FUNCTION, SPECIFYING THR MIN AND MAX WORD FREQUENCY FILTER\n",
    "news_vector = vectorize.fit_transform(train_news_list) # TRANSFORMING THE TRAINING DATASET INTO WORD FREQUENCY TRANFORMATION\n",
    "print( \"THE TABLE OF FREQUENCY WORD DISTRIBUTION\" , news_vector.shape)\n",
    "\n",
    "lr=LogisticRegression()\n",
    "model = lr.fit(news_vector, train_news[\"Label\"])\n",
    "\n",
    "test_news_list = []\n",
    "for row in range(0,len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:27]))# CONVERT THE TESTING DATASET OF 27 COLUMNS INTO ONE ELEMENT IN THE LIST FOR EACH DAY\n",
    "\n",
    "test_vector = vectorize.transform(test_news_list) # TRANSFORMING THE TESTING DATASET INTO WORD FREQUENCY TRANFORMATION\n",
    "\n",
    "predictions = model.predict(test_vector)\n",
    "\n",
    "pd.crosstab(test_news[\"Label\"], predictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "\n",
    "accuracy1=accuracy_score(test_news['Label'], predictions)\n",
    "print(\"the baseline model accuracy\", accuracy1)\n",
    "\n",
    "words = vectorize.get_feature_names_out()\n",
    "coefficients = model.coef_.tolist()[0]\n",
    "coeffdf = pd.DataFrame({'Word' : words,'Coefficient' : coefficients})  # WORD DISTRIBUTION OF THE MODEL\n",
    "\n",
    "coeffdf = coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
    "print(\"Top ten words according to the baseline model\",coeffdf.head(10))\n",
    "print(\"Last ten words according to the baseline model\",coeffdf.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcef0e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " TFID TRANSFOMATION DATAFRAME SHAPE (1492, 284)\n",
      " Logistics Regression with Bigram and TFID 0.5311871227364185\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>to help</td>\n",
       "      <td>-0.870909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>us and</td>\n",
       "      <td>-0.880771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accused of</td>\n",
       "      <td>-0.952199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>people are</td>\n",
       "      <td>-0.974838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>around the</td>\n",
       "      <td>-1.009906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>south africa</td>\n",
       "      <td>-1.019293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>after being</td>\n",
       "      <td>-1.088850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>to kill</td>\n",
       "      <td>-1.197320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>the country</td>\n",
       "      <td>-1.225612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>up in</td>\n",
       "      <td>-1.273317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Word  Coefficient\n",
       "242       to help    -0.870909\n",
       "262        us and    -0.880771\n",
       "4      accused of    -0.952199\n",
       "153    people are    -0.974838\n",
       "20     around the    -1.009906\n",
       "175  south africa    -1.019293\n",
       "6     after being    -1.088850\n",
       "244       to kill    -1.197320\n",
       "188   the country    -1.225612\n",
       "260         up in    -1.273317"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#author: Shravan Chintha\n",
    "#bi-gram \n",
    "\n",
    "nvectorize = TfidfVectorizer(min_df=0.05, max_df=0.85,ngram_range=(2,2)) # DEFINING THE TFID TRANSFORMATION FUNCTION\n",
    "news_nvector = nvectorize.fit_transform(train_news_list)\n",
    "\n",
    "print(\" TFID TRANSFOMATION DATAFRAME SHAPE\",news_nvector.shape)\n",
    "\n",
    "nmodel = lr.fit(news_nvector, train_news[\"Label\"])\n",
    "\n",
    "test_news_list = []\n",
    "for row in range(0,len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:27])) # CONVERT THE TESTING DATASET OF 27 COLUMNS INTO ONE ELEMENT IN THE LIST FOR EACH DAY\n",
    "ntest_vector = nvectorize.transform(test_news_list)\n",
    "npredictions = nmodel.predict(ntest_vector)\n",
    "\n",
    "pd.crosstab(test_news[\"Label\"], npredictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "\n",
    "accuracy2=accuracy_score(test_news['Label'], npredictions)\n",
    "print(\" Logistics Regression with Bigram and TFID\",accuracy2)\n",
    "\n",
    "nwords = nvectorize.get_feature_names_out()\n",
    "ncoefficients = nmodel.coef_.tolist()[0]\n",
    "ncoeffdf = pd.DataFrame({'Word' : nwords, \n",
    "                        'Coefficient' : ncoefficients})\n",
    "ncoeffdf = ncoeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
    "ncoeffdf.head(10)\n",
    "ncoeffdf.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7150550b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest with tfid and bigram 0.5372233400402414\n"
     ]
    }
   ],
   "source": [
    "#author: Shravan Chintha\n",
    "#random forest - bigram\n",
    "\n",
    "nvectorize = TfidfVectorizer(min_df=0.01, max_df=0.95,ngram_range=(2,2))\n",
    "news_nvector = nvectorize.fit_transform(train_news_list)\n",
    "\n",
    "rfmodel = RandomForestClassifier(random_state=55)  #DEFINNG THE RANDOM FOREST MODEL\n",
    "rfmodel = rfmodel.fit(news_nvector, train_news[\"Label\"])\n",
    "test_news_list = []\n",
    "for row in range(0,len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:27]))\n",
    "ntest_vector = nvectorize.transform(test_news_list)\n",
    "\n",
    "rfpredictions = rfmodel.predict(ntest_vector)\n",
    "accuracyrf = accuracy_score(test_news['Label'], rfpredictions)\n",
    "print(\"Random forest with tfid and bigram\", accuracyrf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2baeece9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy:  0.5291750503018109\n"
     ]
    }
   ],
   "source": [
    "#author: Shravan Chintha\n",
    "#Naive Bayes\n",
    "\n",
    "nvectorize = TfidfVectorizer(min_df=0.05, max_df=0.8,ngram_range=(2,2)) #DEFINING THE NAIVE BAYS MODEL\n",
    "news_nvector = nvectorize.fit_transform(train_news_list)\n",
    "\n",
    "nbmodel = MultinomialNB(alpha=0.5)\n",
    "nbmodel = nbmodel.fit(news_nvector, train_news[\"Label\"])\n",
    "\n",
    "test_news_list = []\n",
    "for row in range(0,len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:27])) # CONVERT THE TESTING DATASET OF 27 COLUMNS INTO ONE ELEMENT IN THE LIST FOR EACH DAY\n",
    "ntest_vector = nvectorize.transform(test_news_list)\n",
    "\n",
    "nbpredictions = nbmodel.predict(ntest_vector)\n",
    "nbaccuracy=accuracy_score(test_news['Label'], nbpredictions)\n",
    "print(\"Naive Bayes accuracy: \",nbaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c2c69a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CONFUSION MATRIX OF THE GRADIANT BOOSTING  [[ 92 148]\n",
      " [ 74 183]]\n",
      "Gradient Boosting accuracy:  0.5533199195171026\n"
     ]
    }
   ],
   "source": [
    "#author: Shravan Chintha\n",
    "#Gradient Boosting Classifier\n",
    "\n",
    "gbmodel = GradientBoostingClassifier(random_state=52)  # DEFINING THE GARDIANT BOOSTING MODEL\n",
    "gbmodel = gbmodel.fit(news_nvector, train_news[\"Label\"])\n",
    "test_news_list = []\n",
    "for row in range(0,len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:27]))\n",
    "ntest_vector = nvectorize.transform(test_news_list)\n",
    "\n",
    "gbpredictions = gbmodel.predict(ntest_vector.toarray())\n",
    "gbaccuracy = accuracy_score(test_news['Label'], gbpredictions)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\" CONFUSION MATRIX OF THE GRADIANT BOOSTING \", confusion_matrix(test_news['Label'], gbpredictions))\n",
    "\n",
    "print(\"Gradient Boosting accuracy: \",gbaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df5db5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1492, 569061)\n",
      "TRIGARAM ACCURACY 0.5171026156941649\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TfidfVectorizer' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m accuracy3\u001b[38;5;241m=\u001b[39maccuracy_score(test_news[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabel\u001b[39m\u001b[38;5;124m'\u001b[39m], n3predictions)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTRIGARAM ACCURACY\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy3)\n\u001b[1;32m---> 22\u001b[0m n3words \u001b[38;5;241m=\u001b[39m n3vectorize\u001b[38;5;241m.\u001b[39mget_feature_names()\n\u001b[0;32m     23\u001b[0m n3coefficients \u001b[38;5;241m=\u001b[39m n3model\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mtolist()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     24\u001b[0m n3coeffdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m : n3words, \n\u001b[0;32m     25\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCoefficient\u001b[39m\u001b[38;5;124m'\u001b[39m : n3coefficients})\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TfidfVectorizer' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "#author: Adithya Job\n",
    "#trigram\n",
    "\n",
    "n3vectorize = TfidfVectorizer(min_df=0.0004, max_df=0.115,ngram_range=(3,3)) # DEFINING THE TFID , TRIGRAM MODEL\n",
    "news_n3vector = n3vectorize.fit_transform(train_news_list)\n",
    "\n",
    "print(news_n3vector.shape)\n",
    "\n",
    "n3model = lr.fit(news_n3vector, train_news[\"Label\"])\n",
    "\n",
    "test_news_list = []\n",
    "for row in range(0,len(test_news.index)):\n",
    "    test_news_list.append(' '.join(str(x) for x in test_news.iloc[row,2:27])) # CONVERT THE TESTING DATASET OF 27 COLUMNS INTO ONE ELEMENT IN THE LIST FOR EACH DAY\n",
    "n3test_vector = n3vectorize.transform(test_news_list)\n",
    "n3predictions = n3model.predict(n3test_vector)\n",
    "\n",
    "pd.crosstab(test_news[\"Label\"], n3predictions, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n",
    "\n",
    "accuracy3=accuracy_score(test_news['Label'], n3predictions)\n",
    "print(\"TRIGARAM ACCURACY\", accuracy3)\n",
    "\n",
    "n3words = n3vectorize.get_feature_names()\n",
    "n3coefficients = n3model.coef_.tolist()[0]\n",
    "n3coeffdf = pd.DataFrame({'Word' : n3words, \n",
    "                        'Coefficient' : n3coefficients})\n",
    "n3coeffdf = n3coeffdf.sort_values(['Coefficient', 'Word'], ascending=[0, 1])\n",
    "print(\"trigram top ten word distibution\", n3coeffdf.head(10))\n",
    "print(\"trigram last ten word distibution\", n3coeffdf.tail(10))    # trigram model word distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9221f985",
   "metadata": {},
   "outputs": [],
   "source": [
    "#author: Adithya Job\n",
    "### sentiment analysis\n",
    "\n",
    "train_sentiment=train_news\n",
    "test_sentiment = test_news\n",
    "train_sentiment =train_sentiment.drop(['Date', 'Label'], axis=1)\n",
    "for column in train_sentiment:\n",
    "    train_sentiment[column]=train_sentiment[column].apply(analize_sentiment)  #converting the train headlines into polarity scores\n",
    "train_sentiment = train_sentiment+10  # removing negative co:efficient from the datset for better performance\n",
    "\n",
    "test_sentiment =test_sentiment.drop(['Date', 'Label'], axis=1)\n",
    "for column in test_sentiment:\n",
    "    test_sentiment[column]=test_sentiment[column].apply(analize_sentiment) # converting the test headlines into ploarity \n",
    "test_sentiment=test_sentiment+10 # removing negative co:efficient from the datset for better performance\n",
    "\n",
    "XGB_model= XGBClassifier()  # training the polarity score datset with DIJA \n",
    "gradiant=XGB_model.fit(train_sentiment, train_news['Label'])\n",
    "y_pred= gradiant.predict(test_sentiment)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(test_news['Label'], y_pred))\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Sentiment Accuracy\",accuracy_score(test_news['Label'], y_pred))\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"f1_score__\",f1_score(test_news['Label'], y_pred, average='weighted'))\n",
    "\n",
    "######################END####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307b7e55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
